{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"competition","sourceId":100890,"databundleVersionId":12488425},{"sourceType":"datasetVersion","sourceId":12476254,"datasetId":9,"databundleVersionId":13052206},{"sourceType":"datasetVersion","sourceId":12428327,"datasetId":3240808,"databundleVersionId":12997234}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mohamedzakaria91/meta-kaggle?scriptVersionId=250738572\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"kaggle/meta-kaggle\")\n\nprint(\"Path to dataset files:\", path)\nimport kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"kaggle/meta-kaggle-code\")\n\nprint(\"Path to dataset files:\", path)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-15T02:27:34.575134Z","iopub.execute_input":"2025-07-15T02:27:34.575856Z","iopub.status.idle":"2025-07-15T02:27:35.120563Z","shell.execute_reply.started":"2025-07-15T02:27:34.575831Z","shell.execute_reply":"2025-07-15T02:27:35.119526Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/meta-kaggle\nPath to dataset files: /kaggle/input/meta-kaggle-code\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Meta-Kaggle Hackathon 2025 | Mohamed Zakaria\n\n# ðŸ“¦ 1. Imports & Data Loading\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport warnings\nimport lightgbm as lgb\nimport shap\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import Ridge\n\nwarnings.filterwarnings('ignore')\n\n# Efficient, professional loading\nDATA_DIR = \"/kaggle/input/meta-kaggle\"\nREQUIRED_FILES = {\n    \"Notebooks\": \"Kernels.csv\",\n    \"NotebookVersions\": \"KernelVersions.csv\",\n    \"NotebookTags\": \"KernelTags.csv\",\n    \"Users\": \"Users.csv\",\n    \"Competitions\": \"Competitions.csv\",\n    \"Submissions\": \"Submissions.csv\"\n}\n\ndata = {}\nfor name, filename in REQUIRED_FILES.items():\n    file_path = os.path.join(DATA_DIR, filename)\n    if os.path.exists(file_path):\n        try:\n            df = pd.read_csv(file_path)\n            if df.empty:\n                print(f\" {filename} exists but is empty.\")\n            else:\n                data[name] = df\n                print(f\"Loaded {name} ({df.shape[0]:,} rows)\")\n        except pd.errors.EmptyDataError:\n            print(f\" {filename} is empty or malformed.\")\n    else:\n        print(f\" {filename} not found. Please add it from the 'Add Data' panel.\")\n\n# Unpack DataFrames\nnotebooks = data[\"Notebooks\"]\nusers = data.get(\"Users\")\nsubmissions = data.get(\"Submissions\")\nnotebook_versions = data.get(\"NotebookVersions\")\n\n# ðŸ§¹ Feature Engineering + Merging\nusers['PerformanceTier'] = users['PerformanceTier'].fillna(0)\nusers['RegisterDate'] = pd.to_datetime(users['RegisterDate'])\nnotebooks['CreationDate'] = pd.to_datetime(notebooks['CreationDate'])\n\nmerged_df = notebooks.merge(users, left_on='AuthorUserId', right_on='Id', how='left', suffixes=('', '_User'))\nmerged_df['TenureDays'] = (merged_df['CreationDate'] - merged_df['RegisterDate']).dt.days\nmerged_df['TotalActivities'] = 1\nmerged_df['CreationHour'] = merged_df['CreationDate'].dt.hour\nmerged_df['WeekendCreation'] = merged_df['CreationDate'].dt.dayofweek >= 5\nmerged_df['ActivityRate'] = merged_df['TotalActivities'] / (merged_df['TenureDays'] + 30)\nmerged_df['HighPerformer'] = (merged_df['PerformanceTier'] >= 4).astype(int)\nmerged_df['RecentActivity'] = (merged_df['TenureDays'] < 7).astype(int)\nmerged_df['PeakHour'] = merged_df['CreationHour'].between(12, 18).astype(int)\nmerged_df['EliteUser'] = (merged_df['PerformanceTier'] >= 5).astype(int)\n\n# ðŸ“ˆ Target Variable from Submissions (via KernelVersionId â†’ KernelId)\nif submissions is not None and \"PublicScoreLeaderboardDisplay\" in submissions.columns and notebook_versions is not None:\n    if \"KernelCurrentVersionId\" in notebooks.columns:\n        version_map = notebooks[[\"Id\", \"KernelCurrentVersionId\"]].rename(columns={\"Id\": \"NotebookId\", \"KernelCurrentVersionId\": \"KernelVersionId\"})\n        notebook_versions = notebook_versions.rename(columns={\"Id\": \"KernelVersionId\"})\n        version_map = version_map.merge(notebook_versions[[\"KernelVersionId\", \"KernelId\"]], on=\"KernelVersionId\", how=\"left\")\n        submissions = submissions.merge(version_map, on=\"KernelVersionId\", how=\"left\")\n        valid_scores = submissions.dropna(subset=[\"PublicScoreLeaderboardDisplay\", \"KernelId\"])\n        valid_scores = valid_scores[valid_scores[\"PublicScoreLeaderboardDisplay\"] > 0]\n        leaderboard_scores = valid_scores.groupby(\"KernelId\")[\"PublicScoreLeaderboardDisplay\"].mean().reset_index()\n        leaderboard_scores.columns = [\"KernelId\", \"LeaderboardScore\"]\n        merged_df = merged_df.merge(leaderboard_scores, left_on=\"Id\", right_on=\"KernelId\", how=\"left\")\n        merged_df = merged_df.dropna(subset=[\"LeaderboardScore\"])\n        merged_df['LogScore'] = np.log1p(merged_df['LeaderboardScore'])\n        y = merged_df[\"LogScore\"]\n        print(\"Using log leaderboard score as target\")\n    else:\n        y = np.random.rand(len(merged_df)) * 10\n        print(\"Missing KernelCurrentVersionId â€” using fallback target\")\nelse:\n    y = np.random.rand(len(merged_df)) * 10\n    print(\"No valid leaderboard score found, using random target\")\n\n# Feature Selection\nfeatures = [\n    'TenureDays', 'PerformanceTier', 'TotalActivities',\n    'CreationHour', 'WeekendCreation', 'ActivityRate',\n    'HighPerformer', 'RecentActivity', 'PeakHour', 'EliteUser'\n]\nX = merged_df[features].fillna(0)\n\n# Time-Based Validation\nlatest_date = merged_df['CreationDate'].max()\nval_cutoff = latest_date - pd.Timedelta(days=60)\ntrain_mask = merged_df['CreationDate'] < val_cutoff\nval_mask = ~train_mask\n\nX_train, y_train = X[train_mask], y[train_mask]\nX_val, y_val = X[val_mask], y[val_mask]\n\n# Model Ensembling (LightGBM + RandomForest + Ridge + GradientBoosting)\nlgb_model = lgb.LGBMRegressor(**{\n    'objective': 'regression',\n    'learning_rate': 0.01,\n    'num_leaves': 31,\n    'max_depth': -1,\n    'min_data_in_leaf': 20,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8,\n    'verbosity': -1,\n    'random_state': 42\n}, n_estimators=1000)\nlgb_model.fit(X_train, y_train)\nrf_model = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42)\nrf_model.fit(X_train, y_train)\nridge_model = Ridge(alpha=1.0)\nridge_model.fit(X_train, y_train)\ngb_model = GradientBoostingRegressor(n_estimators=200, learning_rate=0.01, random_state=42)\ngb_model.fit(X_train, y_train)\n\n# Averaging Predictions\ny_pred_ensemble = (lgb_model.predict(X_val) + rf_model.predict(X_val) + ridge_model.predict(X_val) + gb_model.predict(X_val)) / 4\n\n# Metrics\nrmse = mean_squared_error(y_val, y_pred_ensemble, squared=False)\nmae = mean_absolute_error(y_val, y_pred_ensemble)\nr2 = r2_score(y_val, y_pred_ensemble)\n\nprint(f\"\\n RMSE: {rmse:.4f} (Log Leaderboard Score)\")\nprint(f\" MAE: {mae:.4f}\")\nprint(f\" RÂ²: {r2:.4f}\")\n\n# SHAP Explainability\nexplainer = shap.Explainer(lgb_model)\nshap_values = explainer(X_val)\nshap.plots.beeswarm(shap_values)\n\n#  Simulated Leaderboard Ranking Distribution\nmerged_df['PredictedLogScore'] = lgb_model.predict(X)\nmerged_df['PredictedScore'] = np.expm1(merged_df['PredictedLogScore'])\nmerged_df['SimulatedRank'] = merged_df['PredictedScore'].rank(ascending=False)\n\nplt.figure(figsize=(10, 5))\nsns.histplot(merged_df['SimulatedRank'], bins=50, kde=True)\nplt.title(\"Simulated Leaderboard Rank Distribution\")\nplt.xlabel(\"Rank\")\nplt.ylabel(\"Notebook Count\")\nplt.show()\n\n# Interaction Analysis\ninteraction_matrix = pd.DataFrame(shap_values.values, columns=X_val.columns)\ncorrelation = interaction_matrix.corr()\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation, annot=True, cmap=\"coolwarm\")\nplt.title(\"SHAP Interaction Feature Correlations\")\nplt.show()\n\n# Export Results\nif 'Id' in merged_df.columns:\n    merged_df[['Id', 'PredictedScore']].to_csv(\"notebook_score_predictions.csv\", index=False)\n    print(\"Predictions saved to 'notebook_score_predictions.csv'\")\nelse:\n    print(\"'Id' column missing. Could not export predictions.\")\n","metadata":{"execution":{"iopub.status.busy":"2025-07-16T03:54:00.972356Z","iopub.execute_input":"2025-07-16T03:54:00.972922Z","iopub.status.idle":"2025-07-16T04:23:25.975443Z","shell.execute_reply.started":"2025-07-16T03:54:00.972892Z","shell.execute_reply":"2025-07-16T04:23:25.974289Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"What drives a Kaggle notebook to the top of the leaderboard?\n\nIn this deep dive into the Meta-Kaggle dataset, we explore notebook popularity using real user, submission, and competition data. This notebook includes:\n\nExploratory analysis of notebook and author behavior\n\nFeature engineering on user tenure, activity, and notebook metadata\n\nModel ensembling (LightGBM, Random Forest, Ridge, GBoost)\n\nSHAP explainability & feature interaction heatmaps\n\nLeaderboard ranking prediction from public scores\n\nExported predictions for submission or sharing\n\nThis work aims to simulate leaderboard outcomes and uncover the hidden signals behind high-impact notebooks.\n\n","metadata":{}}]}